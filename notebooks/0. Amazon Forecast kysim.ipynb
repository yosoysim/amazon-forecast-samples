{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  First let us setup Amazon Forecast<a class=\"anchor\" id=\"setup\">\n",
    "\n",
    "This section sets up the permissions and relevant endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util.fcst_utils import *\n",
    "from util.hint import hint\n",
    "import warnings, boto3, s3fs, json\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wil create a boto3 session. Boto3 allows us to interact with all of the AWS services via python.\n",
    "The region for the session is the defaulted region for your account. You can choose any of the 6 regions where the forecast service is available.\n",
    "\n",
    "Once we have the boto3 session, we csan create the clients for forecast and forecast query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get boto3 session and s3 client\n",
    "session = boto3.Session()  # can specifify region here i.e. region='us-west-2'\n",
    "\n",
    "# get the s3, forecast and forecast-query clients\n",
    "forecast = session.client('forecast')\n",
    "forecast_query = session.client('forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the role arn when creating various forecast entities, the unique s3 bucket where all our forecast data will be stored and the project prefix for all the forecast entities we will be creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = get_or_create_role_arn()\n",
    "account_id = session.client('sts').get_caller_identity().get('Account')\n",
    "project=f\"forecastKE\"  # 숫자, 기호 등 안 됨\n",
    "\n",
    "bucket_name=\"<your-bucket-name>\" # bucket_name=\"forecast-kysim\"\n",
    "s3_data_path = f\"s3://{bucket_name}/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the parameters that we will pass to the Forecast service that determine how to process the time series data. This includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_frequency = \"D\" # hourly frequency data\n",
    "forecast_horizon = 10  # forecast 24 hours into future\n",
    "timestamp_format = \"yyyy-MM-dd\" # timestamp format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preparing the Datasets<a class=\"anchor\" id=\"prepare\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kcc demo.csv\", dtype = object)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['actual_pax'].astype('int')\n",
    "df['hol_yn'] = df['hol_yn'].astype('int')\n",
    "df['datetime'] = pd.to_datetime(df['op_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['item_id']=='KE0023YC']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take about two and a half week's of hourly data for demonstration, just for the purpose that there's no missing data in the whole range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[-2*7*24-24*3:]\n",
    "df_small['item_id'] = \"KE0023YC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the time series first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.plot(x='datetime', y='count', figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target time series seem to have a drop over weekends. Next let's plot both the target time series and the related time series that indicates whether today is a `workday` or not. More precisely, $r_t = 1$ if $t$ is a work day and 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_small.plot(x='datetime', y='count', ax=ax);\n",
    "#ax2 = ax.twinx()\n",
    "#df_small.plot(x='datetime', y='hol_yn', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df_small[['item_id', 'datetime', 'count']][:-10]\n",
    "#related_df = df_small[['item_id', 'datetime', 'hol_yn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the length of the related time series is equal to the length of the target time series plus the forecast horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_df), len(related_df))\n",
    "assert len(target_df) + 10 == len(related_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check whether there are \"holes\" in the related time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(related_df) == len(pd.date_range(\n",
    "    start=list(related_df['datetime'])[0],\n",
    "    end=list(related_df['datetime'])[-1],\n",
    "    freq=timeseries_frequency\n",
    ")), \"missing entries in the related time series\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, and we plot both time series again. As it can be seen, the related time series (indicator of whether the current day is a workday or not) is longer than the target time series.  The binary working day indicator feature is a good example of a related time series, since it is known at all future time points.  Other examples of related time series include holiday and promotion features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "ax = plt.gca()\n",
    "target_df.plot(x='datetime', y='count', ax=ax);\n",
    "#ax2 = ax.twinx()\n",
    "#related_df.plot(x='datetime', y='hol_yn', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(\"../data/ke0023yc_target.csv\", index= False, header = False)\n",
    "#related_df.to_csv(\"../data/ke0023yc_related.csv\", index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have this bucket `amazon-forecast-data-{account_id}`, create it first on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync data to s3\n",
    "# '!' is used to make calls to the os shell\n",
    "# where we then us the aws command line\n",
    "#!aws s3 mb s3://$bucket_name\n",
    "!aws s3 sync ../data $s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a. Creating a Dataset Group<a class=\"anchor\" id=\"create\">\n",
    "First let's create a dataset group and then update it later to add our datasets. Since this is **RETAIL** use case we will specify that as the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = f\"{project}_dataset_group\"\n",
    "\n",
    "\n",
    "print(dataset_group)\n",
    "create_dataset_group_response = forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                                          DatasetGroupName=dataset_group,\n",
    "                                                          DatasetArns=[])\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "\n",
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b. Creating a Target Dataset<a class=\"anchor\" id=\"target\">\n",
    "Now we will define a target time series. This is a required dataset to use the service. For this exmaple, the number of items sold, or `demand` is the target value we will be forecasting. \n",
    "  \n",
    "First, we specify the name and schema of our dataset. Make sure the order of the attributes (columns) matches the raw data in the files. We follow the same three attribute format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{project}_target_dataset\"\n",
    "schema = {\n",
    "    \"Attributes\": [\n",
    "        {\n",
    "            \"AttributeName\": \"item_id\", \n",
    "            \"AttributeType\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"timestamp\", \n",
    "            \"AttributeType\": \"timestamp\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"demand\", \n",
    "            \"AttributeType\": \"float\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a schema and name we can create the target data set. This only sets up the definition of the dataset. No data has been imported to Forecast yet. Data import will happen later when we create the import jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                               DatasetType='TARGET_TIME_SERIES',\n",
    "                               DatasetName=name,\n",
    "                               DataFrequency=timeseries_frequency,\n",
    "                               Schema=schema\n",
    ")\n",
    "\n",
    "target_dataset_arn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=target_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c. Creating a Related Dataset<a class=\"anchor\" id=\"related\">\n",
    "Now we will create a related time series dataset using the related price data for the items. The method call is very simliar to the above, except you will have `price` instead of `demand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{project}_related_dataset\"\n",
    "schema = {\n",
    "    \"Attributes\": [\n",
    "        {\n",
    "            \"AttributeName\": \"item_id\", \n",
    "            \"AttributeType\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"timestamp\", \n",
    "            \"AttributeType\": \"timestamp\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"price\", \n",
    "            \"AttributeType\": \"float\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                               DatasetType='RELATED_TIME_SERIES',\n",
    "                               DatasetName=name,\n",
    "                               DataFrequency=timeseries_frequency,\n",
    "                               Schema=schema\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "related_dataset_arn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=related_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d. Updating the dataset group with the datasets we created<a class=\"anchor\" id=\"update\">\n",
    "You can have multiple datasets under the same dataset group. Update it with the datasets we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_arns = [target_dataset_arn, related_dataset_arn]\n",
    "dataset_arns = [target_dataset_arn] # Non-daily flight 문제\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)\n",
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2e. Creating a Target Time Series Dataset Import Job<a class=\"anchor\" id=\"targetImport\">\n",
    "    \n",
    "Now that we that we have defined the target time series, will still need to create an import job to actually load the data into Amazon Forecast from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f\"{s3_data_path}/ke0023yc_target.csv\"\n",
    "\n",
    "response = forecast.create_dataset_import_job(\n",
    "    DatasetImportJobName=dataset_group,\n",
    "    DatasetArn=target_dataset_arn,\n",
    "    DataSource= {\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        } \n",
    "    },\n",
    "    TimestampFormat= timestamp_format\n",
    ")\n",
    "\n",
    "target_dataset_import_job_arn = response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2f. Creating a Related Time Series Dataset Import Job<a class=\"anchor\" id=\"relatedImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f\"{s3_data_path}/ke0023yc_related.csv\"\n",
    "response = forecast.create_dataset_import_job(\n",
    "    DatasetImportJobName=dataset_group,\n",
    "    DatasetArn=related_dataset_arn,\n",
    "    DataSource= {\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        } \n",
    "    },\n",
    "    TimestampFormat= timestamp_format\n",
    ")\n",
    "related_dataset_import_job_arn = response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to wait for all the import jobs to finish. We will use a simple blocking `wait` method that checks the import job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=target_dataset_import_job_arn)))\n",
    "#assert(wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=related_dataset_import_job_arn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_algorithm_arn = 'arn:aws:forecast:::algorithm/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a. Choosing DeepAR+<a class=\"anchor\" id=\"DeepAR\">\n",
    "    \n",
    "Amazon Forecast DeepAR+ is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNNs). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series, and then use that model to extrapolate the time series into the future. In many applications, however, you have many similar time series across a set of cross-sectional units. These time-series groupings demand different products, server loads, and requests for web pages. In this case, it can be beneficial to train a single model jointly over all of the time series. DeepAR+ takes this approach. When your dataset contains hundreds of feature time series, the DeepAR+ algorithm outperforms the standard ARIMA and ETS methods. You can also use the trained model for generating forecasts for new time series that are similar to the ones it has been trained on.\n",
    "\n",
    "For more on DeepAR+, see the [Amazon Forecast Doumentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = f'{base_algorithm_arn}Deep_AR_Plus'\n",
    "predictor_name = f'{project}_Deep_AR_Pls'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "predictor_arn_deep_ar = response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b.  Choosing Prophet<a class=\"anchor\" id=\"prophet\">\n",
    "    \n",
    "Prophet is a popular local Bayesian structural time series model. The Amazon Forecast Prophet algorithm uses the Prophet class of the Python implementation of Prophet.\n",
    "\n",
    "#### How Prophet Works\n",
    "Prophet is especially useful for datasets that:\n",
    "\n",
    "* Contain an extended time period (months or years) of detailed historical observations (hourly, daily, or weekly)\n",
    "* Have multiple strong seasonalities\n",
    "* Include previously known important, but irregular, events\n",
    "* Have missing data points or large outliers\n",
    "* Have non-linear growth trends that are approaching a limit\n",
    "\n",
    "Prophet is an additive regression model with a piecewise linear or logistic growth curve trend. It includes a yearly seasonal component modeled using Fourier series and a weekly seasonal component modeled using dummy variables.\n",
    "\n",
    "For more information, see the [Amazon Forecast Documentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-prophet.html).\n",
    "\n",
    "Prophet Hyperparameters and Related Time Series\n",
    "Amazon Forecast uses the default Prophet hyperparameters. Prophet also supports related time-series as features, provided to Amazon Forecast in the related time-series CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = f'{base_algorithm_arn}Prophet'\n",
    "predictor_name = f'{project}_Prophet'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "predictor_arn_prophet = response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Want to try another Predictor?\n",
    "If you would like to try different predictors, check out the [Amazon Forecast Documentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html), and create another predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn =f'{base_algorithm_arn}ARIMA'\n",
    "predictor_name=f'{project}_ARIMA'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "\n",
    "predictor_arn_arima= response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to wait for both predictors to complete training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "forecast.describe_predictor(PredictorArn=predictor_arn_prophet)\n",
    "\n",
    "wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_arima))\n",
    "forecast.describe_predictor(PredictorArn=predictor_arn_arima)\n",
    "\n",
    "wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Error Metrics\n",
    "\n",
    "Now that we have trained predictors, we can get the error metrics for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)\n",
    "print(error_metrics_deep_ar_plus)\n",
    "\n",
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)\n",
    "print(error_metrics_prophet)\n",
    "\n",
    "error_metrics_other = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_arima)\n",
    "print(error_metrics_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")\n",
    "\n",
    "arima_metrics = extract_summary_metrics(error_metrics_other, \"ARIMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [deep_ar_metrics, prophet_metrics, arima_metrics] # arima added\n",
    "#if predictor_arn_arima:\n",
    "#    metrics.append(arima_metrics)\n",
    "    \n",
    "pd.concat(metrics) \\\n",
    "        .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, if you only have a handful of time series (in this case, only 1) with a small number of examples, the neural network models (DeepAR+) are not the best choice. Here, we clearly see that DeepAR+ behaves worse than Prophet in the case of a single time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Creating a Forecast<a class=\"anchor\" id=\"forecast\">\n",
    "\n",
    "The `create_forecast` method uses the predictor to create a forecast. In the response, you will get the Amazon Resource Name (ARN) of the forecast. You use this ARN to retrieve and export the forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_deep_ar_plus'\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_deep_ar\n",
    ")\n",
    "\n",
    "forecast_arn_deep_ar = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a forecast using the prophet dataset and the optional predictor if you created one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_prophet'\n",
    "\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_prophet\n",
    ")\n",
    "\n",
    "forecast_arn_prophet = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_arima'\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_other\n",
    ")\n",
    "forecast_arn_arima = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to wait for the forecasts to be finish being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar))\n",
    "forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar)\n",
    "\n",
    "wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_prophet))\n",
    "forecast.describe_forecast(ForecastArn=forecast_arn_prophet)\n",
    "\n",
    "wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_arima))\n",
    "forecast.describe_forecast(ForecastArn=forecast_arn_arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Querying the Forecasts<a class=\"anchor\" id=\"query\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the forecasts that have been created,  use the following parameters.\n",
    "\n",
    "* **start-date** and **end-date** – Specifies an optional date range to retrieve the forecast for. If you don't specify these parameters, the operation returns the entire forecast for bike_12.\n",
    "* **filters** – Specifies the item_id filter to retrieve the electricity forecast for bike_12.\n",
    "\n",
    "Because this is an hourly forecast, the response shows hourly forecast values. In the response, note the following:\n",
    "\n",
    "* **mean** – For the specific date and time, the mean is the predicted mean value.\n",
    "* **p90, p50, and p10** – Specify the confidence level that the actual value will be below the listed value at the specified date and time. \n",
    "\n",
    "For more information about this operation, see [QueryForecast](https://docs.aws.amazon.com/forecast/latest/dg/API_forecastquery_QueryForecast.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 'ke0023yc'\n",
    "\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is a json structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a utility function already created for you to plot the actual values against the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "fname = f'../data/ke0023yc.csv'\n",
    "actual = load_exact_sol(fname, item_id)\n",
    "\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"DeepAR Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now query the forecast for prophet=, and if you created it, your optional predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_prophet,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")\n",
    "\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"Prophet Forecast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_arima,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"ARIMA Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Exporting your Forecasts<a class=\"anchor\" id=\"export\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_deep_ar_plus'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_deep_ar = response['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create uour own export job for the prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_prophet'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_prophet,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_prophet = response['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_arima'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_arima,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_arima = response['ForecastExportJobArn']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
