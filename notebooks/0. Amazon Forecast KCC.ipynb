{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  First let us setup Amazon Forecast<a class=\"anchor\" id=\"setup\">\n",
    "\n",
    "This section sets up the permissions and relevant endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util.fcst_utils import *\n",
    "from util.hint import hint\n",
    "import warnings, boto3, s3fs, json\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wil create a boto3 session. Boto3 allows us to interact with all of the AWS services via python.\n",
    "The region for the session is the defaulted region for your account. You can choose any of the 6 regions where the forecast service is available.\n",
    "\n",
    "Once we have the boto3 session, we csan create the clients for forecast and forecast query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get boto3 session and s3 client\n",
    "session = boto3.Session()  # can specifify region here i.e. region='us-west-2'\n",
    "\n",
    "# get the s3, forecast and forecast-query clients\n",
    "forecast = session.client('forecast')\n",
    "forecast_query = session.client('forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the role arn when creating various forecast entities, the unique s3 bucket where all our forecast data will be stored and the project prefix for all the forecast entities we will be creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = get_or_create_role_arn()\n",
    "account_id = session.client('sts').get_caller_identity().get('Account')\n",
    "project=f\"forecastKE\"  # 숫자, 기호 등 안 됨\n",
    "\n",
    "bucket_name=\"<your-bucket-name>\" # bucket_name=\"forecast-kysim\"\n",
    "s3_data_path = f\"s3://{bucket_name}/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the parameters that we will pass to the Forecast service that determine how to process the time series data. This includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_frequency = \"D\" # hourly frequency data\n",
    "forecast_horizon = 24  # forecast 24 hours into future\n",
    "timestamp_format = \"yyyy-MM-dd\" # timestamp format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preparing the Datasets<a class=\"anchor\" id=\"prepare\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kcc demo.csv\", dtype = object)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = bike_df['actual_pax'].astype('float')\n",
    "df['hol_yn'] = bike_df['hol_yn'].astype('float')\n",
    "df['datetime'] = pd.to_datetime(df['op_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take about two and a half week's of hourly data for demonstration, just for the purpose that there's no missing data in the whole range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[-2*7*24-24*3:]\n",
    "df_small['item_id'] = \"KE0023YC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the time series first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.plot(x='datetime', y='count', figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target time series seem to have a drop over weekends. Next let's plot both the target time series and the related time series that indicates whether today is a `workday` or not. More precisely, $r_t = 1$ if $t$ is a work day and 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "bike_df_small.plot(x='datetime', y='count', ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "bike_df_small.plot(x='datetime', y='hol_yn', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that to use the related time series, we need to ensure that the related time series covers the whole target time series, as well as the future values as specified by the forecast horizon. More precisely, we need to make sure:\n",
    "```\n",
    "len(related time series) >= len(target time series) + forecast horizon\n",
    "```\n",
    "Basically, all items need to have data start at or before the item start date, and have data until the forecast horizon (i.e. the latest end date across all items + forecast horizon).  Additionally, there should be no missing values in the related time series. The following picture illustrates the desired logic. \n",
    "\n",
    "<img src=\"BlogImages/rts_viz.png\">\n",
    "\n",
    "For more details regarding how to prepare your Related Time Series dataset, please refer to the public documentation <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html\">here</a>. \n",
    "\n",
    "Suppose in this particular example, we wish to forecast for the next 24 hours, and thus we generate the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df_small[['item_id', 'datetime', 'count']][:-10]\n",
    "related_df = df_small[['item_id', 'datetime', 'hol_yn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the length of the related time series is equal to the length of the target time series plus the forecast horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_df), len(related_df))\n",
    "assert len(target_df) + 10 == len(related_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check whether there are \"holes\" in the related time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(related_df) == len(pd.date_range(\n",
    "    start=list(related_df['datetime'])[0],\n",
    "    end=list(related_df['datetime'])[-1],\n",
    "    freq=timeseries_frequency\n",
    ")), \"missing entries in the related time series\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, and we plot both time series again. As it can be seen, the related time series (indicator of whether the current day is a workday or not) is longer than the target time series.  The binary working day indicator feature is a good example of a related time series, since it is known at all future time points.  Other examples of related time series include holiday and promotion features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "ax = plt.gca()\n",
    "target_df.plot(x='datetime', y='count', ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "related_df.plot(x='datetime', y='hol_yn', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(\"../data/ke0023yc_target.csv\", index= False, header = False)\n",
    "related_df.to_csv(\"../data/ke0023yc_related.csv\", index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have this bucket `amazon-forecast-data-{account_id}`, create it first on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync data to s3\n",
    "# '!' is used to make calls to the os shell\n",
    "# where we then us the aws command line\n",
    "#!aws s3 mb s3://$bucket_name\n",
    "!aws s3 sync ../data $s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a. Creating a Dataset Group<a class=\"anchor\" id=\"create\">\n",
    "First let's create a dataset group and then update it later to add our datasets. Since this is **RETAIL** use case we will specify that as the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = f\"{project}_dataset_group\"\n",
    "\n",
    "\n",
    "print(dataset_group)\n",
    "create_dataset_group_response = forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                                          DatasetGroupName=dataset_group,\n",
    "                                                          DatasetArns=[])\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "\n",
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b. Creating a Target Dataset<a class=\"anchor\" id=\"target\">\n",
    "Now we will define a target time series. This is a required dataset to use the service. For this exmaple, the number of items sold, or `demand` is the target value we will be forecasting. \n",
    "  \n",
    "First, we specify the name and schema of our dataset. Make sure the order of the attributes (columns) matches the raw data in the files. We follow the same three attribute format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{project}_target_dataset\"\n",
    "schema = {\n",
    "    \"Attributes\": [\n",
    "        {\n",
    "            \"AttributeName\": \"item_id\", \n",
    "            \"AttributeType\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"timestamp\", \n",
    "            \"AttributeType\": \"timestamp\"\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"demand\", \n",
    "            \"AttributeType\": \"float\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a schema and name we can create the target data set. This only sets up the definition of the dataset. No data has been imported to Forecast yet. Data import will happen later when we create the import jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                               DatasetType='TARGET_TIME_SERIES',\n",
    "                               DatasetName=name,\n",
    "                               DataFrequency=timeseries_frequency,\n",
    "                               Schema=schema\n",
    ")\n",
    "\n",
    "target_dataset_arn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=target_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c. Creating a Related Dataset<a class=\"anchor\" id=\"related\">\n",
    "Now we will create a related time series dataset using the related price data for the items. The method call is very simliar to the above, except you will have `price` instead of `demand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hint('2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{project}_related_dataset\"\n",
    "schema = # TODO\n",
    "\n",
    "response = # TODO\n",
    "\n",
    "\n",
    "\n",
    "related_dataset_arn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=related_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d. Updating the dataset group with the datasets we created<a class=\"anchor\" id=\"update\">\n",
    "You can have multiple datasets under the same dataset group. Update it with the datasets we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = [target_dataset_arn, related_dataset_arn]\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)\n",
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2e. Creating a Target Time Series Dataset Import Job<a class=\"anchor\" id=\"targetImport\">\n",
    "    \n",
    "Now that we that we have defined the target time series, will still need to create an import job to actually load the data into Amazon Forecast from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f\"{s3_data_path}/ke0023yc_target.csv\"\n",
    "\n",
    "response = forecast.create_dataset_import_job(\n",
    "    DatasetImportJobName=dataset_group,\n",
    "    DatasetArn=target_dataset_arn,\n",
    "    DataSource= {\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "            \"RoleArn\": role_arn\n",
    "        } \n",
    "    },\n",
    "    TimestampFormat= timestamp_format\n",
    ")\n",
    "\n",
    "target_dataset_import_job_arn = response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2f. Creating a Related Time Series Dataset Import Job<a class=\"anchor\" id=\"relatedImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint('2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f\"{s3_data_path}/ke0023yc_related.csv\"\n",
    "response = # TODO\n",
    "related_dataset_import_job_arn = response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to wait for all the import jobs to finish. We will use a simple blocking `wait` method that checks the import job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=target_dataset_import_job_arn)))\n",
    "assert(wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=related_dataset_import_job_arn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Choosing an Algorithm <a class=\"anchor\" id=\"algo\">\n",
    "\n",
    "Once the datasets are specified with the corresponding schema, Amazon Forecast will automatically aggregate all the relevant pieces of information for each item, such as sales, price, promotions, as well as categorical attributes, and generate the desired dataset. Next, one can choose an algorithm (forecasting model) and evaluate how well this particular algorithm works on this dataset. The following graph gives a high-level overview of the forecasting models.\n",
    "<img src=\"BlogImages/recipes.png\">\n",
    "<img src=\"BlogImages/pred_details.png\">\n",
    "\n",
    "\n",
    "Amazon Forecast provides several state-of-the-art forecasting algorithms including classic forecasting methods such as ETS, ARIMA, Prophet and deep learning approaches such as DeepAR+. Classical forecasting methods, such as Autoregressive Integrated Moving Average (ARIMA) or Exponential Smoothing (ETS), fit a single model to each individual time series, and then use that model to extrapolate the time series into the future. Amazon's Non-Parametric Time Series (NPTS) forecaster also fits a single model to each individual time series.  Unlike the naive or seasonal naive forecasters that use a fixed time index (the previous index $T-1$ or the past season $T - \\tau$) as the prediction for time step $T$, NPTS randomly samples a time index $t \\in \\{0, \\dots T-1\\}$ in the past to generate a sample for the current time step $T$.\n",
    "\n",
    "In many applications, you may encounter many similar time series across a set of cross-sectional units. Examples of such time series groupings are demand for different products, server loads, and requests for web pages. In this case, it can be beneficial to train a single model jointly over all of these time series. DeepAR+ takes this approach, outperforming the standard ARIMA and ETS methods when your dataset contains hundreds of related time series. The trained model can also be used for generating forecasts for new time series that are similar to the ones it has been trained on. While deep learning approaches can outperform standard methods, this is only possible when there is sufficient data available for training. It is not true for example when one trains a neural network with a time-series contains only a few dozens of observations. Amazon Forecast provides the best of two worlds allowing users to either choose a specific algorithm or let Amazon Forecast automatically perform model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_algorithm_arn = 'arn:aws:forecast:::algorithm/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a. Choosing DeepAR+<a class=\"anchor\" id=\"DeepAR\">\n",
    "    \n",
    "Amazon Forecast DeepAR+ is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNNs). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series, and then use that model to extrapolate the time series into the future. In many applications, however, you have many similar time series across a set of cross-sectional units. These time-series groupings demand different products, server loads, and requests for web pages. In this case, it can be beneficial to train a single model jointly over all of the time series. DeepAR+ takes this approach. When your dataset contains hundreds of feature time series, the DeepAR+ algorithm outperforms the standard ARIMA and ETS methods. You can also use the trained model for generating forecasts for new time series that are similar to the ones it has been trained on.\n",
    "\n",
    "For more on DeepAR+, see the [Amazon Forecast Doumentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = f'{base_algorithm_arn}Deep_AR_Plus'\n",
    "predictor_name = f'{project}_Deep_AR_Pls'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "predictor_arn_deep_ar = response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b.  Choosing Prophet<a class=\"anchor\" id=\"prophet\">\n",
    "    \n",
    "Prophet is a popular local Bayesian structural time series model. The Amazon Forecast Prophet algorithm uses the Prophet class of the Python implementation of Prophet.\n",
    "\n",
    "#### How Prophet Works\n",
    "Prophet is especially useful for datasets that:\n",
    "\n",
    "* Contain an extended time period (months or years) of detailed historical observations (hourly, daily, or weekly)\n",
    "* Have multiple strong seasonalities\n",
    "* Include previously known important, but irregular, events\n",
    "* Have missing data points or large outliers\n",
    "* Have non-linear growth trends that are approaching a limit\n",
    "\n",
    "Prophet is an additive regression model with a piecewise linear or logistic growth curve trend. It includes a yearly seasonal component modeled using Fourier series and a weekly seasonal component modeled using dummy variables.\n",
    "\n",
    "For more information, see the [Amazon Forecast Documentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-prophet.html).\n",
    "\n",
    "Prophet Hyperparameters and Related Time Series\n",
    "Amazon Forecast uses the default Prophet hyperparameters. Prophet also supports related time-series as features, provided to Amazon Forecast in the related time-series CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = f'{base_algorithm_arn}Prophet'\n",
    "predictor_name = f'{project}_Prophet'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "predictor_arn_prophet = response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Want to try another Predictor?\n",
    "If you would like to try different predictors, check out the [Amazon Forecast Documentation](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html), and create another predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn_other=f'{base_algorithm_arn}NPTS'\n",
    "predictor_name=f'{project}_NPTS'\n",
    "\n",
    "response = forecast.create_predictor(\n",
    "    PredictorName = predictor_name,\n",
    "    AlgorithmArn = algorithm_arn,\n",
    "    ForecastHorizon = forecast_horizon,\n",
    "    PerformAutoML = False,\n",
    "    PerformHPO = False,\n",
    "    InputDataConfig = {'DatasetGroupArn': dataset_group_arn},\n",
    "    FeaturizationConfig = {'ForecastFrequency': timeseries_frequency}\n",
    ")\n",
    "\n",
    "\n",
    "predictor_arn_other= response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to wait for both predictors to complete training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "forecast.describe_predictor(PredictorArn=predictor_arn_prophet)\n",
    "\n",
    "if predictor_arn_other:\n",
    "    wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_other))\n",
    "    forecast.describe_predictor(PredictorArn=predictor_arn_other)\n",
    "\n",
    "wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Error Metrics\n",
    "\n",
    "Now that we have trained predictors, we can get the error metrics for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)\n",
    "print(error_metrics_deep_ar_plus)\n",
    "\n",
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)\n",
    "print(error_metrics_prophet)\n",
    "\n",
    "if predictor_arn_other:\n",
    "    error_metrics_other = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_other)\n",
    "    print(error_metrics_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")\n",
    "\n",
    "if predictor_arn_other:\n",
    "    other_metrics = extract_summary_metrics(error_metrics_other, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [deep_ar_metrics, prophet_metrics]\n",
    "if predictor_arn_other:\n",
    "    metrics.append(other_metrics)\n",
    "    \n",
    "pd.concat(metrics) \\\n",
    "        .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, if you only have a handful of time series (in this case, only 1) with a small number of examples, the neural network models (DeepAR+) are not the best choice. Here, we clearly see that DeepAR+ behaves worse than Prophet in the case of a single time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Creating a Forecast<a class=\"anchor\" id=\"forecast\">\n",
    "\n",
    "The `create_forecast` method uses the predictor to create a forecast. In the response, you will get the Amazon Resource Name (ARN) of the forecast. You use this ARN to retrieve and export the forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_deep_ar_plus'\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_deep_ar\n",
    ")\n",
    "\n",
    "forecast_arn_deep_ar = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a forecast using the prophet dataset and the optional predictor if you created one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint('5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_prophet'\n",
    "\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_prophet\n",
    ")\n",
    "\n",
    "forecast_arn_prophet = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f'{project}_other'\n",
    "response = forecast.create_forecast(\n",
    "    ForecastName=forecast_name,\n",
    "    PredictorArn=predictor_arn_other\n",
    ")\n",
    "forecast_arn_other = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to wait for the forecasts to be finish being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar))\n",
    "forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar)\n",
    "\n",
    "wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_prophet))\n",
    "forecast.describe_forecast(ForecastArn=forecast_arn_prophet)\n",
    "\n",
    "if predictor_arn_other:\n",
    "    wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_other))\n",
    "    forecast.describe_forecast(ForecastArn=forecast_arn_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Querying the Forecasts<a class=\"anchor\" id=\"query\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the forecasts that have been created,  use the following parameters.\n",
    "\n",
    "* **start-date** and **end-date** – Specifies an optional date range to retrieve the forecast for. If you don't specify these parameters, the operation returns the entire forecast for bike_12.\n",
    "* **filters** – Specifies the item_id filter to retrieve the electricity forecast for bike_12.\n",
    "\n",
    "Because this is an hourly forecast, the response shows hourly forecast values. In the response, note the following:\n",
    "\n",
    "* **mean** – For the specific date and time, the mean is the predicted mean value.\n",
    "* **p90, p50, and p10** – Specify the confidence level that the actual value will be below the listed value at the specified date and time. \n",
    "\n",
    "For more information about this operation, see [QueryForecast](https://docs.aws.amazon.com/forecast/latest/dg/API_forecastquery_QueryForecast.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 'bike_12'\n",
    "\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is a json structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a utility function already created for you to plot the actual values against the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "fname = f'../data/ke0023yc.csv'\n",
    "actual = load_exact_sol(fname, item_id)\n",
    "\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"DeepAR Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now query the forecast for prophet=, and if you created it, your optional predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_prophet,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")\n",
    "\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"Prophet Forecast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_other,\n",
    "    Filters={\"item_id\": item_id}\n",
    ")\n",
    "plot_forecasts(response, actual)\n",
    "plt.title(\"Other Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Exporting your Forecasts<a class=\"anchor\" id=\"export\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_deep_ar_plus'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_deep_ar = response['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create uour own export job for the prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_prophet'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_prophet,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_prophet = response['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{project}_forecast_export_other'\n",
    "s3_path = f\"{s3_data_path}/{name}\"\n",
    "\n",
    "response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=name,\n",
    "    ForecastArn=forecast_arn_other,\n",
    "    Destination={\n",
    "        \"S3Config\" : {\n",
    "            \"Path\": s3_path,\n",
    "             \"RoleArn\": role_arn\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "forecast_export_arn_other = response['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Cleaning up your Resources<a class=\"anchor\" id=\"cleanup\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the above steps, we can start to cleanup the resources we created. All delete jobs, except for `delete_dataset_group` are asynchronous, so we have added the helpful `wait_till_delete` function. \n",
    "Resource Limits documented <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/limits.html\">here</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast export for both algorithms\n",
    "wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_deep_ar))\n",
    "wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_prophet))\n",
    "if forecast_export_arn_other:\n",
    "    wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast for both algorithms\n",
    "wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_deep_ar))\n",
    "wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_prophet))\n",
    "if forecast_export_arn_other:\n",
    "    wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_arn_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete predictor for both algorithms\n",
    "wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn_deep_ar))\n",
    "wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn_prophet))\n",
    "if forecast_export_arn_other:\n",
    "    wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = predictor_arn_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series dataset import jobs\n",
    "wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=target_dataset_import_job_arn))\n",
    "wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=related_dataset_import_job_arn))\n",
    "wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=item_metadata_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series datasets\n",
    "wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=target_dataset_arn))\n",
    "wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=related_dataset_arn))\n",
    "wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=item_metadata_dataset_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataset group\n",
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
